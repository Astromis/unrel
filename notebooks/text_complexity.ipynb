{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rusyll.rusyll import token_to_syllables\n",
    "from nltk import word_tokenize, sent_tokenize \n",
    "from nltk.tokenize import SyllableTokenizer\n",
    "import numpy as np\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Будучи ребёнком, Кусуноки думал, что его ждёт великое будущее. Однако прошло десять лет и наивные детские мечты сменились суровой реальностью. Жизнь Кусуноки сера и безрадостна. Двадцатилетний парень работает в кафе, и денег ему не хватает даже на еду. Распродавая свои книги и диски, он узнаёт от знакомого букиниста об особом магазине, в котором скупают жизни людей. При этом стоимость меняется в зависимости от человека. Восприняв это как несмешную шутку, парень решает всё же сам всё проверить. Так как за свою жизнь японец в среднем тратит около 200—300 миллионов иен, примерно на такую сумму Кусуноки и рассчитывает. Однако оставшуюся Кусуноки жизнь нельзя назвать счастливой и в ней не будет никаких достижений, поэтому, основываясь на наличии таких факторов, как счастье, актуализация и вклад, он обнаруживает, что за каждый год магазин готов предложить лишь минимальную цену в десять тысяч иен. Хотя Кусуноки и рассчитывал долго прожить, в настоящем исходе ему остаётся не более тридцати лет. В итоге Кусуноки решает воспользоваться этой возможностью и продаёт тридцать лет своей жизни, оставив себе только три месяца. На оставшийся срок к нему приставляют наблюдательницу Мияги, которая должна гарантировать, что её подопечный не будет создавать проблем другим людям. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flash_readibility(text, lang, type):\n",
    "    if lang not in [\"english\", \"russian\"]:\n",
    "        raise NotImplementedError(lang)\n",
    "    tokens = word_tokenize(text, lang)\n",
    "    sents = sent_tokenize(text, lang)\n",
    "    mean_sent_len = np.mean([len(x.split(' ')) for x in sents])\n",
    "    if lang == \"russian\":\n",
    "        mean_word_len = np.mean([len(token_to_syllables(x)) for x in tokens])\n",
    "        if type == \"flash\":\n",
    "            readibility = 206.836-60.1*mean_word_len-1.3*mean_sent_len\n",
    "        elif type == \"flash-kinside\":\n",
    "            readibility = 0.5*mean_sent_len + 8.4*mean_word_len - 15.59\n",
    "\n",
    "    else:\n",
    "        SSP = SyllableTokenizer()\n",
    "        mean_word_len = np.mean([len(SSP.tokenize(x)) for x in tokens])\n",
    "        if type == \"flash\":\n",
    "            readibility = 206.836-84.6*mean_word_len-1.015*mean_sent_len\n",
    "        elif type == \"flash-kinside\":\n",
    "            readibility = 0.39*mean_sent_len + 11.8*mean_word_len - 15.59\n",
    "    return readibility\n",
    "\n",
    "def fog_index(text, lang):\n",
    "    tokens = word_tokenize(text, lang)\n",
    "    if len(tokens) < 100:\n",
    "        warnings.warn(f\"Text should contain at least 100 tokens. You have {len(tokens)}\")\n",
    "    middle = len(tokens) // 2\n",
    "    selected_tokens = tokens[middle-50:middle] + tokens[middle:middle+50]\n",
    "    sents = sent_tokenize(' '.join(selected_tokens), lang)\n",
    "    mean_sent_len = np.mean([len(x.split(' ')) for x in sents])\n",
    "    if lang == \"russian\":\n",
    "        word_syll = [len(token_to_syllables(x)) for x in tokens]\n",
    "    else:\n",
    "        SSP = SyllableTokenizer()\n",
    "        word_syll = [len(SSP.tokenize(x)) for x in tokens]\n",
    "    complex_words = list(filter(lambda x: x > 2, word_syll))\n",
    "    complex_words_frac = len(complex_words) / len(selected_tokens)\n",
    "    if lang == \"english\":\n",
    "        fog_index = (mean_sent_len + complex_words_frac) * 0.4\n",
    "    else:\n",
    "        fog_index = (1.25*mean_sent_len + 0.24*complex_words_frac) * 0.4\n",
    "    return fog_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.073920000000001"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fog_index(text, \"russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2146118721461185"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7 (default, Sep 16 2021, 13:09:58) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11df7bf060af0b04d565c59b1f69f91a2aa058dd22dbda95c21af2764a0cb933"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
